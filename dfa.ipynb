{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # ベクトルxの各要素から最大値を引く（オーバーフロー防止のため）\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    # 要素ごとのexpの和で各要素を割り、確率として解釈できるようにする\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def initialize_bp_params():\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(400, 784) * 0.01\n",
    "    b1 = np.zeros((400, 1))\n",
    "    W2 = np.random.randn(400, 400) * 0.01\n",
    "    b2 = np.zeros((400, 1))\n",
    "    W3 = np.random.randn(400, 400) * 0.01\n",
    "    b3 = np.zeros((400, 1))\n",
    "    W4 = np.random.randn(10, 400) * 0.01\n",
    "    b4 = np.zeros((10, 1))\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "                    \"b1\": b1,\n",
    "                    \"W2\": W2,\n",
    "                    \"b2\": b2,\n",
    "                    \"W3\": W3,\n",
    "                    \"b3\": b3,\n",
    "                    \"W4\": W4,\n",
    "                    \"b4\": b4}\n",
    "\n",
    "    return params\n",
    "\n",
    "def initialize_dfa_params():\n",
    "    np.random.seed(1)\n",
    "    W1 = np.zeros((400, 784))\n",
    "    b1 = np.zeros((400, 1))\n",
    "    W2 = np.zeros((400, 400))\n",
    "    b2 = np.zeros((400, 1))\n",
    "    W3 = np.zeros((400, 400))\n",
    "    b3 = np.zeros((400, 1))\n",
    "    W4 = np.zeros((10, 400))\n",
    "    b4 = np.zeros((10, 1))\n",
    "\n",
    "    B1 = np.random.randn(400, 10) * 0.01\n",
    "    B2 = np.random.randn(400, 10) * 0.01\n",
    "    B3 = np.random.randn(400, 10) * 0.01\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "                    \"b1\": b1,\n",
    "                    \"W2\": W2,\n",
    "                    \"b2\": b2,\n",
    "                    \"W3\": W3,\n",
    "                    \"b3\": b3,\n",
    "                    \"W4\": W4,\n",
    "                    \"b4\": b4,\n",
    "                    \"B1\": B1,\n",
    "                    \"B2\": B2,\n",
    "                    \"B3\": B3}\n",
    "\n",
    "    return params\n",
    "\n",
    "def forward_propagation(X, params):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "    b3 = params[\"b3\"]\n",
    "    W4 = params[\"W4\"]\n",
    "    b4 = params[\"b4\"]\n",
    "\n",
    "    A1 = np.dot(W1, X) + b1\n",
    "    H1 = np.tanh(A1)\n",
    "    A2 = np.dot(W2, H1) + b2\n",
    "    H2 = np.tanh(A2)\n",
    "    A3 = np.dot(W3, H2) + b3\n",
    "    H3 = np.tanh(A3)\n",
    "    AY = np.dot(W4, H3) + b4\n",
    "    Y_ = softmax(AY)\n",
    "\n",
    "    cache = {\"A1\": A1,\n",
    "            \"H1\": H1,\n",
    "            \"A2\": A2,\n",
    "            \"H2\": H2,\n",
    "            \"A3\": A3,\n",
    "            \"H3\": H3,\n",
    "            \"AY\": AY,\n",
    "            \"Y_\": Y_}\n",
    "\n",
    "    return Y_, cache\n",
    "\n",
    "\n",
    "def backward_propagation(params, cache, X, Y):\n",
    "    # batch size\n",
    "    N = X.shape[1]\n",
    "\n",
    "    W1 = params[\"W1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    W3 = params[\"W3\"]\n",
    "    W4 = params[\"W4\"]\n",
    "\n",
    "    H1 = cache[\"H1\"]\n",
    "    H2 = cache[\"H2\"]\n",
    "    H3 = cache[\"H3\"]\n",
    "    Y_ = cache[\"Y_\"]\n",
    "\n",
    "    dY_ = Y_ - Y\n",
    "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
    "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
    "\n",
    "    dH3 = np.dot(W4.T, dY_)\n",
    "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
    "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
    "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
    "\n",
    "    dH2 = np.dot(W3.T, dA3)\n",
    "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
    "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
    "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
    "\n",
    "    dH1 = np.dot(W2.T, dA2)\n",
    "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
    "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
    "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
    "\n",
    "    gradients = {\"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2,\n",
    "                \"dW3\": dW3,\n",
    "                \"db3\": db3,\n",
    "                \"dW4\": dW4,\n",
    "                \"db4\": db4}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def direct_feedback_alignment(params, cache, X, Y):\n",
    "    # batch size\n",
    "    N = X.shape[1]\n",
    "\n",
    "    B1 = params[\"B1\"]\n",
    "    B2 = params[\"B2\"]\n",
    "    B3 = params[\"B3\"]\n",
    "\n",
    "    H1 = cache[\"H1\"]\n",
    "    H2 = cache[\"H2\"]\n",
    "    H3 = cache[\"H3\"]\n",
    "    Y_ = cache[\"Y_\"]\n",
    "\n",
    "    dY_ = Y_ - Y\n",
    "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
    "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
    "\n",
    "    dH3 = np.dot(B3, dY_)\n",
    "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
    "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
    "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
    "\n",
    "    dH2 = np.dot(B2, dY_)\n",
    "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
    "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
    "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
    "\n",
    "    dH1 = np.dot(B1, dY_)\n",
    "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
    "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
    "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
    "\n",
    "    gradients = {\"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2,\n",
    "                \"dW3\": dW3,\n",
    "                \"db3\": db3,\n",
    "                \"dW4\": dW4,\n",
    "                \"db4\": db4}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def update_params(params, grads, learning_rate=0.01):\n",
    "    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
    "    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
    "    params[\"W4\"] -= learning_rate * grads[\"dW4\"]\n",
    "    params[\"b4\"] -= learning_rate * grads[\"db4\"]\n",
    "    return params\n",
    "\n",
    "\n",
    "def compute_loss(Y_, Y):\n",
    "    \n",
    "    N = Y.shape[1]\n",
    "    loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)))\n",
    "    loss = np.squeeze(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(backward_propagation, params, train_loader, test_loader, learning_rate=0.01, num_epochs=100):\n",
    "    train_error_list = []\n",
    "    test_error_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            X = images.view(-1, 28 * 28)\n",
    "            X = np.asarray(X).T\n",
    "            Y = np.eye(10)[labels]\n",
    "            Y = np.asarray(Y).T\n",
    "\n",
    "            labels_gpu = np.asarray(labels)\n",
    "\n",
    "            Y_, cache = forward_propagation(X, params)\n",
    "            loss = compute_loss(Y_, Y)\n",
    "            grads = backward_propagation(params, cache, X, Y)\n",
    "            params = update_params(params, grads, learning_rate)\n",
    "\n",
    "            total_loss += loss\n",
    "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
    "            total_samples += labels_gpu.shape[0]\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print('Epoch {}, iteration {}, loss : {}, accuracy : {}'.format(epoch+1, i+1, total_loss / 100, total_correct/ total_samples))\n",
    "                total_loss = 0\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "\n",
    "        train_error_rate = (1 - (total_correct / total_samples)) * 100\n",
    "        train_error_list.append(train_error_rate)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            X = images.view(-1, 28 * 28)\n",
    "            X = np.asarray(X).T\n",
    "            Y = np.eye(10)[labels]\n",
    "            Y = np.asarray(Y).T\n",
    "\n",
    "            labels_gpu = np.asarray(labels)\n",
    "\n",
    "            Y_, cache = forward_propagation(X, params)\n",
    "            loss = compute_loss(Y_, Y)\n",
    "\n",
    "            total_loss += loss\n",
    "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
    "            total_samples += labels_gpu.shape[0]\n",
    "\n",
    "        test_error_rate = (1 - (total_correct / total_samples)) * 100\n",
    "        test_error_list.append(test_error_rate)\n",
    "\n",
    "        print('Test data - After epoch {}, loss : {}, accuracy : {}'.format(epoch+1, total_loss / len(test_loader), total_correct / total_samples))\n",
    "    \n",
    "\n",
    "    return params, train_error_list, test_error_list\n",
    "\n",
    "\n",
    "def predict(X, params):\n",
    "    Y_, cache = forward_propagation(X, params)\n",
    "    predictions = np.argmax(Y_, axis=0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return np.sum(predictions == labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    batch_size = 64\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    bp_params = initialize_bp_params()\n",
    "    dfa_params = initialize_dfa_params()\n",
    "    bp_params, bp_train_errors, bp_test_errors = train(backward_propagation, bp_params, train_loader, test_loader, learning_rate=0.01, num_epochs=10)\n",
    "    dfa_params, dfa_train_errors, dfa_test_errors = train(direct_feedback_alignment, dfa_params, train_loader, test_loader, learning_rate=0.01, num_epochs=10)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(bp_train_errors, label='BP train')\n",
    "    plt.plot(bp_test_errors, label='BP test')\n",
    "    plt.plot(dfa_train_errors, label='DFA train')\n",
    "    plt.plot(dfa_test_errors, label='DFA test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
