{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNB2kJd7cdvuD3R5VIfGYeB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/root2116/direct-feedback-alignment/blob/main/dfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NT2T4KaesDK-"
      },
      "outputs": [],
      "source": [
        "import cupy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def softmax(x):\n",
        "    # ベクトルxの各要素から最大値を引く（オーバーフロー防止のため）\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    # 要素ごとのexpの和で各要素を割り、確率として解釈できるようにする\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def initialize_parameters():\n",
        "    np.random.seed(1)\n",
        "    W1 = np.zeros((400, 784))\n",
        "    b1 = np.zeros((400, 1))\n",
        "    W2 = np.zeros((400, 400))\n",
        "    b2 = np.zeros((400, 1))\n",
        "    W3 = np.zeros((400, 400))\n",
        "    b3 = np.zeros((400, 1))\n",
        "    W4 = np.zeros((10, 400))\n",
        "    b4 = np.zeros((10, 1))\n",
        "\n",
        "    B1 = np.random.randn(400, 10) * 0.01\n",
        "    B2 = np.random.randn(400, 10) * 0.01\n",
        "    B3 = np.random.randn(400, 10) * 0.01\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                    \"b1\": b1,\n",
        "                    \"W2\": W2,\n",
        "                    \"b2\": b2,\n",
        "                    \"W3\": W3,\n",
        "                    \"b3\": b3,\n",
        "                    \"W4\": W4,\n",
        "                    \"b4\": b4,\n",
        "                    \"B1\": B1,\n",
        "                    \"B2\": B2,\n",
        "                    \"B3\": B3}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    W4 = parameters[\"W4\"]\n",
        "    b4 = parameters[\"b4\"]\n",
        "\n",
        "    A1 = np.dot(W1, X) + b1\n",
        "    H1 = np.tanh(A1)\n",
        "    A2 = np.dot(W2, H1) + b2\n",
        "    H2 = np.tanh(A2)\n",
        "    A3 = np.dot(W3, H2) + b3\n",
        "    H3 = np.tanh(A3)\n",
        "    AY = np.dot(W4, H3) + b4\n",
        "    Y_ = softmax(AY)\n",
        "\n",
        "    cache = {\"A1\": A1,\n",
        "            \"H1\": H1,\n",
        "            \"A2\": A2,\n",
        "            \"H2\": H2,\n",
        "            \"A3\": A3,\n",
        "            \"H3\": H3,\n",
        "            \"AY\": AY,\n",
        "            \"Y_\": Y_}\n",
        "\n",
        "    return Y_, cache\n",
        "\n",
        "\n",
        "def direct_feedback_alignment(parameters, cache, X, Y):\n",
        "    # batch size\n",
        "    N = X.shape[1]\n",
        "\n",
        "    B1 = parameters[\"B1\"]\n",
        "    B2 = parameters[\"B2\"]\n",
        "    B3 = parameters[\"B3\"]\n",
        "\n",
        "    H1 = cache[\"H1\"]\n",
        "    H2 = cache[\"H2\"]\n",
        "    H3 = cache[\"H3\"]\n",
        "    Y_ = cache[\"Y_\"]\n",
        "\n",
        "    dY_ = Y_ - Y\n",
        "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
        "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
        "\n",
        "    dH3 = np.dot(B3, dY_)\n",
        "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
        "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
        "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
        "\n",
        "    dH2 = np.dot(B2, dY_)\n",
        "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
        "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
        "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
        "\n",
        "    dH1 = np.dot(B1, dY_)\n",
        "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
        "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
        "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dW1\": dW1,\n",
        "                \"db1\": db1,\n",
        "                \"dW2\": dW2,\n",
        "                \"db2\": db2,\n",
        "                \"dW3\": dW3,\n",
        "                \"db3\": db3,\n",
        "                \"dW4\": dW4,\n",
        "                \"db4\": db4}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate=0.01):\n",
        "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "    parameters[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "    parameters[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "    parameters[\"W4\"] -= learning_rate * grads[\"dW4\"]\n",
        "    parameters[\"b4\"] -= learning_rate * grads[\"db4\"]\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def compute_loss(Y_, Y):\n",
        "\n",
        "    N = Y.shape[1]\n",
        "    loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)))\n",
        "    loss = np.squeeze(loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def model(train_loader, test_loader, learning_rate=0.01, num_epochs=100):\n",
        "    parameters = initialize_parameters()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, parameters)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "            grads = direct_feedback_alignment(parameters, cache, X, Y)\n",
        "            parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print('Epoch {}, iteration {}, loss : {}, accuracy : {}'.format(epoch+1, i+1, total_loss / 100, total_correct/ total_samples))\n",
        "                total_loss = 0\n",
        "                total_correct = 0\n",
        "                total_samples = 0\n",
        "\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, parameters)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "        print('Test data - After epoch {}, loss : {}, accuracy : {}'.format(epoch+1, total_loss / len(test_loader), total_correct / total_samples))\n",
        "\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def predict(X, parameters):\n",
        "    Y_, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.argmax(Y_, axis=0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return np.sum(predictions == labels) / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    batch_size = 64\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    parameters = model(train_loader, test_loader, learning_rate=0.01, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq6MzxfHsHko",
        "outputId": "a2e5c9a0-3114-456e-88ad-e7bd3d93b8a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, iteration 100, loss : 2.302463397341995, accuracy : 0.11140625\n",
            "Epoch 1, iteration 200, loss : 2.302335049448346, accuracy : 0.10828125\n",
            "Epoch 1, iteration 300, loss : 2.3019446357066555, accuracy : 0.1178125\n",
            "Epoch 1, iteration 400, loss : 2.3017477650043316, accuracy : 0.11234375\n",
            "Epoch 1, iteration 500, loss : 2.3012529358083698, accuracy : 0.1078125\n",
            "Epoch 1, iteration 600, loss : 2.290384509385251, accuracy : 0.151875\n",
            "Epoch 1, iteration 700, loss : 2.2307191252596006, accuracy : 0.25625\n",
            "Epoch 1, iteration 800, loss : 2.059526245338341, accuracy : 0.30546875\n",
            "Epoch 1, iteration 900, loss : 1.7976610747519128, accuracy : 0.3684375\n",
            "Test data - After epoch 1, loss : 1.565549474651598, accuracy : 0.4549\n",
            "Epoch 2, iteration 100, loss : 1.478846270592119, accuracy : 0.48953125\n",
            "Epoch 2, iteration 200, loss : 1.288680916806956, accuracy : 0.560625\n",
            "Epoch 2, iteration 300, loss : 1.1688561595504903, accuracy : 0.5896875\n",
            "Epoch 2, iteration 400, loss : 1.066881414933913, accuracy : 0.64875\n",
            "Epoch 2, iteration 500, loss : 1.011626901525619, accuracy : 0.69015625\n",
            "Epoch 2, iteration 600, loss : 0.8984475076258674, accuracy : 0.73125\n",
            "Epoch 2, iteration 700, loss : 0.8688233178105136, accuracy : 0.74765625\n",
            "Epoch 2, iteration 800, loss : 0.7990573239467227, accuracy : 0.774375\n",
            "Epoch 2, iteration 900, loss : 0.7789704866549927, accuracy : 0.76953125\n",
            "Test data - After epoch 2, loss : 0.7066735392423145, accuracy : 0.7979\n",
            "Epoch 3, iteration 100, loss : 0.6837469043446504, accuracy : 0.808125\n",
            "Epoch 3, iteration 200, loss : 0.6795048136893784, accuracy : 0.80328125\n",
            "Epoch 3, iteration 300, loss : 0.668390686240955, accuracy : 0.8071875\n",
            "Epoch 3, iteration 400, loss : 0.6298447618240521, accuracy : 0.80953125\n",
            "Epoch 3, iteration 500, loss : 0.6162379261214762, accuracy : 0.81640625\n",
            "Epoch 3, iteration 600, loss : 0.5800097532038907, accuracy : 0.82671875\n",
            "Epoch 3, iteration 700, loss : 0.5864457251267922, accuracy : 0.8290625\n",
            "Epoch 3, iteration 800, loss : 0.5603129079273942, accuracy : 0.83328125\n",
            "Epoch 3, iteration 900, loss : 0.5581987905353857, accuracy : 0.834375\n",
            "Test data - After epoch 3, loss : 0.528654562590148, accuracy : 0.8439\n",
            "Epoch 4, iteration 100, loss : 0.5243190681796192, accuracy : 0.845\n",
            "Epoch 4, iteration 200, loss : 0.5377384460179171, accuracy : 0.83984375\n",
            "Epoch 4, iteration 300, loss : 0.5079695526496218, accuracy : 0.8446875\n",
            "Epoch 4, iteration 400, loss : 0.5247171283866728, accuracy : 0.84484375\n",
            "Epoch 4, iteration 500, loss : 0.5009889913033866, accuracy : 0.85765625\n",
            "Epoch 4, iteration 600, loss : 0.49928259830759053, accuracy : 0.8546875\n",
            "Epoch 4, iteration 700, loss : 0.4730334595686571, accuracy : 0.86203125\n",
            "Epoch 4, iteration 800, loss : 0.48252065735258126, accuracy : 0.85703125\n",
            "Epoch 4, iteration 900, loss : 0.4648943987877234, accuracy : 0.86484375\n",
            "Test data - After epoch 4, loss : 0.4524818084944836, accuracy : 0.8693\n",
            "Epoch 5, iteration 100, loss : 0.46807692526156797, accuracy : 0.8625\n",
            "Epoch 5, iteration 200, loss : 0.44474870387734683, accuracy : 0.8646875\n",
            "Epoch 5, iteration 300, loss : 0.4466802695201165, accuracy : 0.87484375\n",
            "Epoch 5, iteration 400, loss : 0.46881361771271124, accuracy : 0.86625\n",
            "Epoch 5, iteration 500, loss : 0.4653493640430513, accuracy : 0.86109375\n",
            "Epoch 5, iteration 600, loss : 0.43949821783387477, accuracy : 0.8725\n",
            "Epoch 5, iteration 700, loss : 0.41937162708072223, accuracy : 0.8790625\n",
            "Epoch 5, iteration 800, loss : 0.4251370358268911, accuracy : 0.87296875\n",
            "Epoch 5, iteration 900, loss : 0.43942951610619346, accuracy : 0.87140625\n",
            "Test data - After epoch 5, loss : 0.4109959119224478, accuracy : 0.8813\n",
            "Epoch 6, iteration 100, loss : 0.42139071845750353, accuracy : 0.87796875\n",
            "Epoch 6, iteration 200, loss : 0.4169666021933391, accuracy : 0.8765625\n",
            "Epoch 6, iteration 300, loss : 0.4055965689364433, accuracy : 0.87859375\n",
            "Epoch 6, iteration 400, loss : 0.4166669975738032, accuracy : 0.875625\n",
            "Epoch 6, iteration 500, loss : 0.40498743938259174, accuracy : 0.8803125\n",
            "Epoch 6, iteration 600, loss : 0.40351460116288984, accuracy : 0.883125\n",
            "Epoch 6, iteration 700, loss : 0.4156477281362671, accuracy : 0.88203125\n",
            "Epoch 6, iteration 800, loss : 0.41520310413315087, accuracy : 0.87515625\n",
            "Epoch 6, iteration 900, loss : 0.4053359451020211, accuracy : 0.8809375\n",
            "Test data - After epoch 6, loss : 0.3845614611540899, accuracy : 0.8871\n",
            "Epoch 7, iteration 100, loss : 0.40616381183813155, accuracy : 0.88125\n",
            "Epoch 7, iteration 200, loss : 0.40018292499137026, accuracy : 0.88328125\n",
            "Epoch 7, iteration 300, loss : 0.3796410390070767, accuracy : 0.8865625\n",
            "Epoch 7, iteration 400, loss : 0.3746325511913742, accuracy : 0.88640625\n",
            "Epoch 7, iteration 500, loss : 0.3905381163194566, accuracy : 0.884375\n",
            "Epoch 7, iteration 600, loss : 0.3960957293423059, accuracy : 0.88125\n",
            "Epoch 7, iteration 700, loss : 0.397126934930425, accuracy : 0.8878125\n",
            "Epoch 7, iteration 800, loss : 0.3839242871516788, accuracy : 0.88515625\n",
            "Epoch 7, iteration 900, loss : 0.3810439830976849, accuracy : 0.88578125\n",
            "Test data - After epoch 7, loss : 0.3634906810734507, accuracy : 0.8915\n",
            "Epoch 8, iteration 100, loss : 0.3720064741635897, accuracy : 0.8903125\n",
            "Epoch 8, iteration 200, loss : 0.3699942298821793, accuracy : 0.88796875\n",
            "Epoch 8, iteration 300, loss : 0.3851190616757771, accuracy : 0.88703125\n",
            "Epoch 8, iteration 400, loss : 0.3663372010888712, accuracy : 0.8875\n",
            "Epoch 8, iteration 500, loss : 0.36323421678370726, accuracy : 0.89265625\n",
            "Epoch 8, iteration 600, loss : 0.3745141822287374, accuracy : 0.89\n",
            "Epoch 8, iteration 700, loss : 0.38801370460765044, accuracy : 0.88703125\n",
            "Epoch 8, iteration 800, loss : 0.36097106140146623, accuracy : 0.89046875\n",
            "Epoch 8, iteration 900, loss : 0.3713326936103593, accuracy : 0.88828125\n",
            "Test data - After epoch 8, loss : 0.3501807615301256, accuracy : 0.8962\n",
            "Epoch 9, iteration 100, loss : 0.36930710849790116, accuracy : 0.8909375\n",
            "Epoch 9, iteration 200, loss : 0.37863852825756583, accuracy : 0.88375\n",
            "Epoch 9, iteration 300, loss : 0.3468329879338053, accuracy : 0.89703125\n",
            "Epoch 9, iteration 400, loss : 0.3597411049364516, accuracy : 0.89078125\n",
            "Epoch 9, iteration 500, loss : 0.35756301593189727, accuracy : 0.89796875\n",
            "Epoch 9, iteration 600, loss : 0.34792988953093856, accuracy : 0.89609375\n",
            "Epoch 9, iteration 700, loss : 0.3426997152153573, accuracy : 0.90203125\n",
            "Epoch 9, iteration 800, loss : 0.3441848813216024, accuracy : 0.89546875\n",
            "Epoch 9, iteration 900, loss : 0.37257889023299817, accuracy : 0.89078125\n",
            "Test data - After epoch 9, loss : 0.3385824882277017, accuracy : 0.8991\n",
            "Epoch 10, iteration 100, loss : 0.364736677237843, accuracy : 0.8940625\n",
            "Epoch 10, iteration 200, loss : 0.34264868165400175, accuracy : 0.895\n",
            "Epoch 10, iteration 300, loss : 0.3480370905904385, accuracy : 0.896875\n",
            "Epoch 10, iteration 400, loss : 0.3450507320924393, accuracy : 0.89640625\n",
            "Epoch 10, iteration 500, loss : 0.3162415744271569, accuracy : 0.90359375\n",
            "Epoch 10, iteration 600, loss : 0.3597607748899303, accuracy : 0.8965625\n",
            "Epoch 10, iteration 700, loss : 0.3656393390503892, accuracy : 0.89421875\n",
            "Epoch 10, iteration 800, loss : 0.34801568416057876, accuracy : 0.8978125\n",
            "Epoch 10, iteration 900, loss : 0.3260724188061303, accuracy : 0.90375\n",
            "Test data - After epoch 10, loss : 0.32693887965955915, accuracy : 0.9025\n"
          ]
        }
      ]
    }
  ]
}