{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAyoXzT+ayHresIJl8SBPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/root2116/direct-feedback-alignment/blob/main/dfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 追実験1"
      ],
      "metadata": {
        "id": "U62x1YtgLJl5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NT2T4KaesDK-"
      },
      "outputs": [],
      "source": [
        "import cupy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    # ベクトルxの各要素から最大値を引く（オーバーフロー防止のため）\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    # 要素ごとのexpの和で各要素を割り、確率として解釈できるようにする\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+ np.exp(-x))\n",
        "\n",
        "def initialize_bp_params():\n",
        "    np.random.seed(1)\n",
        "    W1 = np.random.randn(400, 784) * 0.01\n",
        "    b1 = np.zeros((400, 1))\n",
        "    W2 = np.random.randn(400, 400) * 0.01\n",
        "    b2 = np.zeros((400, 1))\n",
        "    W3 = np.random.randn(400, 400) * 0.01\n",
        "    b3 = np.zeros((400, 1))\n",
        "    W4 = np.random.randn(10, 400) * 0.01\n",
        "    b4 = np.zeros((10, 1))\n",
        "\n",
        "    params = {  \"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2,\n",
        "                \"W3\": W3,\n",
        "                \"b3\": b3,\n",
        "                \"W4\": W4,\n",
        "                \"b4\": b4}\n",
        "\n",
        "    return params\n",
        "\n",
        "def initialize_dfa_params():\n",
        "    np.random.seed(1)\n",
        "    W1 = np.zeros((400, 784))\n",
        "    b1 = np.zeros((400, 1))\n",
        "    W2 = np.zeros((400, 400))\n",
        "    b2 = np.zeros((400, 1))\n",
        "    W3 = np.zeros((400, 400))\n",
        "    b3 = np.zeros((400, 1))\n",
        "    W4 = np.zeros((10, 400))\n",
        "    b4 = np.zeros((10, 1))\n",
        "\n",
        "    B1 = np.random.randn(400, 10) * 0.01\n",
        "    B2 = np.random.randn(400, 10) * 0.01\n",
        "    B3 = np.random.randn(400, 10) * 0.01\n",
        "\n",
        "    params = {  \"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2,\n",
        "                \"W3\": W3,\n",
        "                \"b3\": b3,\n",
        "                \"W4\": W4,\n",
        "                \"b4\": b4,\n",
        "                \"B1\": B1,\n",
        "                \"B2\": B2,\n",
        "                \"B3\": B3}\n",
        "\n",
        "    return params\n",
        "\n",
        "def forward_propagation(X, params):\n",
        "    W1 = params[\"W1\"]\n",
        "    b1 = params[\"b1\"]\n",
        "    W2 = params[\"W2\"]\n",
        "    b2 = params[\"b2\"]\n",
        "    W3 = params[\"W3\"]\n",
        "    b3 = params[\"b3\"]\n",
        "    W4 = params[\"W4\"]\n",
        "    b4 = params[\"b4\"]\n",
        "\n",
        "    A1 = np.dot(W1, X) + b1\n",
        "    H1 = np.tanh(A1)\n",
        "    A2 = np.dot(W2, H1) + b2\n",
        "    H2 = np.tanh(A2)\n",
        "    A3 = np.dot(W3, H2) + b3\n",
        "    H3 = np.tanh(A3)\n",
        "    AY = np.dot(W4, H3) + b4\n",
        "    # Y_ = softmax(AY)\n",
        "    Y_ = sigmoid(AY)\n",
        "\n",
        "    cache = {\"A1\": A1,\n",
        "            \"H1\": H1,\n",
        "            \"A2\": A2,\n",
        "            \"H2\": H2,\n",
        "            \"A3\": A3,\n",
        "            \"H3\": H3,\n",
        "            \"AY\": AY,\n",
        "            \"Y_\": Y_}\n",
        "\n",
        "    return Y_, cache\n",
        "\n",
        "\n",
        "def backward_propagation(params, cache, X, Y):\n",
        "    # batch size\n",
        "    N = X.shape[1]\n",
        "\n",
        "    W1 = params[\"W1\"]\n",
        "    W2 = params[\"W2\"]\n",
        "    W3 = params[\"W3\"]\n",
        "    W4 = params[\"W4\"]\n",
        "\n",
        "    H1 = cache[\"H1\"]\n",
        "    H2 = cache[\"H2\"]\n",
        "    H3 = cache[\"H3\"]\n",
        "    Y_ = cache[\"Y_\"]\n",
        "\n",
        "    dY_ = (Y_ - Y)\n",
        "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
        "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
        "\n",
        "    dH3 = np.dot(W4.T, dY_)\n",
        "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
        "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
        "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
        "\n",
        "    dH2 = np.dot(W3.T, dA3)\n",
        "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
        "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
        "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
        "\n",
        "    dH1 = np.dot(W2.T, dA2)\n",
        "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
        "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
        "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dW1\": dW1,\n",
        "                \"db1\": db1,\n",
        "                \"dW2\": dW2,\n",
        "                \"db2\": db2,\n",
        "                \"dW3\": dW3,\n",
        "                \"db3\": db3,\n",
        "                \"dW4\": dW4,\n",
        "                \"db4\": db4}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def direct_feedback_alignment(params, cache, X, Y):\n",
        "    # batch size\n",
        "    N = X.shape[1]\n",
        "\n",
        "    B1 = params[\"B1\"]\n",
        "    B2 = params[\"B2\"]\n",
        "    B3 = params[\"B3\"]\n",
        "\n",
        "    H1 = cache[\"H1\"]\n",
        "    H2 = cache[\"H2\"]\n",
        "    H3 = cache[\"H3\"]\n",
        "    Y_ = cache[\"Y_\"]\n",
        "\n",
        "    dY_ = Y_ - Y\n",
        "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
        "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
        "\n",
        "    dH3 = np.dot(B3, dY_)\n",
        "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
        "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
        "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
        "\n",
        "    dH2 = np.dot(B2, dY_)\n",
        "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
        "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
        "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
        "\n",
        "    dH1 = np.dot(B1, dY_)\n",
        "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
        "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
        "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dW1\": dW1,\n",
        "                \"db1\": db1,\n",
        "                \"dW2\": dW2,\n",
        "                \"db2\": db2,\n",
        "                \"dW3\": dW3,\n",
        "                \"db3\": db3,\n",
        "                \"dW4\": dW4,\n",
        "                \"db4\": db4}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def update_params(params, grads, learning_rate=0.01):\n",
        "    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "    params[\"W4\"] -= learning_rate * grads[\"dW4\"]\n",
        "    params[\"b4\"] -= learning_rate * grads[\"db4\"]\n",
        "    return params\n",
        "\n",
        "\n",
        "def compute_loss(Y_, Y):\n",
        "\n",
        "    N = Y.shape[1]\n",
        "    # loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)))\n",
        "    loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)) + np.multiply(1 - Y, np.log(1 - Y_)))\n",
        "    loss = np.squeeze(loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(backward_propagation, params, train_loader, test_loader, learning_rate=0.01, num_epochs=100):\n",
        "    train_error_list = []\n",
        "    test_error_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, params)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "            grads = backward_propagation(params, cache, X, Y)\n",
        "            params = update_params(params, grads, learning_rate)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print('Epoch {}, iteration {}, loss : {}, accuracy : {}'.format(epoch+1, i+1, total_loss / 100, total_correct/ total_samples))\n",
        "                total_loss = 0\n",
        "\n",
        "\n",
        "        train_error_rate = (1 - (total_correct / total_samples)) * 100\n",
        "        train_error_list.append(train_error_rate)\n",
        "\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, params)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "        test_error_rate = (1 - (total_correct / total_samples)) * 100\n",
        "        test_error_list.append(test_error_rate)\n",
        "\n",
        "        print('Test data - After epoch {}, loss : {}, accuracy : {}'.format(epoch+1, total_loss / len(test_loader), total_correct / total_samples))\n",
        "\n",
        "\n",
        "    return params, train_error_list, test_error_list\n",
        "\n",
        "\n",
        "def predict(X, params):\n",
        "    Y_, cache = forward_propagation(X, params)\n",
        "    predictions = np.argmax(Y_, axis=0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return np.sum(predictions == labels) / len(labels)\n",
        "\n",
        "def extract_features(params, loader):\n",
        "    input_images = []\n",
        "    first_hidden = []\n",
        "    second_hidden = []\n",
        "    third_hidden = []\n",
        "\n",
        "    for images, labels in loader:\n",
        "        X = images.view(-1, 28 * 28)\n",
        "        X = np.asarray(X).T\n",
        "\n",
        "        Y_, cache = forward_propagation(X, params)\n",
        "        input_images.append(X.T)\n",
        "        first_hidden.append(cache[\"H1\"].T)\n",
        "        second_hidden.append(cache[\"H2\"].T)\n",
        "        third_hidden.append(cache[\"H3\"].T)\n",
        "\n",
        "    return np.concatenate(input_images), np.concatenate(first_hidden), np.concatenate(second_hidden), np.concatenate(third_hidden)\n",
        "\n",
        "\n",
        "def plot_tsne(features, title):\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    reduced_features = tsne.fit_transform(features)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], marker='.', s=1)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    batch_size = 64\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    bp_params = initialize_bp_params()\n",
        "    dfa_params = initialize_dfa_params()\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jq6MzxfHsHko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Training with back propagation...\")\n",
        "    bp_params, bp_train_errors, bp_test_errors = train(backward_propagation, bp_params, train_loader, test_loader, learning_rate=0.01, num_epochs=10)"
      ],
      "metadata": {
        "id": "OIl5xg58KEfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Training with direct feedback alignment...\")\n",
        "    dfa_params, dfa_train_errors, dfa_test_errors = train(direct_feedback_alignment, dfa_params, train_loader, test_loader, learning_rate=0.01, num_epochs=10)"
      ],
      "metadata": {
        "id": "DHcQwKH0KG2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    epochs = range(1, len(bp_train_errors) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(epochs, [err.get() for err in bp_train_errors], label='BP train')\n",
        "    plt.plot(epochs, [err.get() for err in bp_test_errors], label='BP test')\n",
        "    plt.plot(epochs, [err.get() for err in dfa_train_errors], label='DFA train')\n",
        "    plt.plot(epochs, [err.get() for err in dfa_test_errors], label='DFA test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error rate(%)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kHdxLvtTJKl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Extracting and plotting features for back propagation...\")\n",
        "    bp_input_images, bp_first_hidden, bp_second_hidden, bp_third_hidden = extract_features(bp_params, test_loader)\n",
        "    plot_tsne(bp_input_images, 'Backpropagation: Input Images')\n",
        "    plot_tsne(bp_first_hidden, 'Backpropagation: First Hidden Layer')\n",
        "    plot_tsne(bp_second_hidden, 'Backpropagation: Second Hidden Layer')\n",
        "    plot_tsne(bp_third_hidden, 'Backpropagation: Third Hidden Layer')\n",
        "\n",
        "    print(\"Extracting and plotting features for direct feedback alignment...\")\n",
        "    dfa_input_images, dfa_first_hidden, dfa_second_hidden, dfa_third_hidden = extract_features(dfa_params, test_loader)\n",
        "    plot_tsne(dfa_input_images, 'Direct Feedback Alignment: Input Images')\n",
        "    plot_tsne(dfa_first_hidden, 'Direct Feedback Alignment: First Hidden Layer')\n",
        "    plot_tsne(dfa_second_hidden, 'Direct Feedback Alignment: Second Hidden Layer')\n",
        "    plot_tsne(dfa_third_hidden, 'Direct Feedback Alignment: Third Hidden Layer')"
      ],
      "metadata": {
        "id": "JjkG9Mi4WGam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 追実験2"
      ],
      "metadata": {
        "id": "PLDScmJRLDKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "def softmax(x):\n",
        "    # ベクトルxの各要素から最大値を引く（オーバーフロー防止のため）\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    # 要素ごとのexpの和で各要素を割り、確率として解釈できるようにする\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+ np.exp(-x))\n",
        "\n",
        "def initialize_bp_params():\n",
        "    np.random.seed(1)\n",
        "    W1 = np.random.randn(50, 784) * 0.01\n",
        "    b1 = np.zeros((50, 1))\n",
        "    W2 = np.random.randn(50, 400) * 0.01\n",
        "    b2 = np.zeros((50, 1))\n",
        "    W3 = np.random.randn(50, 50) * 0.01\n",
        "    b3 = np.zeros((50, 1))\n",
        "    W4 = np.random.randn(10, 50) * 0.01\n",
        "    b4 = np.zeros((10, 1))\n",
        "\n",
        "    params = {  \"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2,\n",
        "                \"W3\": W3,\n",
        "                \"b3\": b3,\n",
        "                \"W4\": W4,\n",
        "                \"b4\": b4}\n",
        "\n",
        "    return params\n",
        "\n",
        "def initialize_dfa_params():\n",
        "    np.random.seed(1)\n",
        "    W1 = np.zeros((50, 784))\n",
        "    b1 = np.zeros((50, 1))\n",
        "    W2 = np.zeros((50, 50))\n",
        "    b2 = np.zeros((50, 1))\n",
        "    W3 = np.zeros((50, 50))\n",
        "    b3 = np.zeros((50, 1))\n",
        "    W4 = np.zeros((10, 50))\n",
        "    b4 = np.zeros((10, 1))\n",
        "\n",
        "    B1 = np.random.randn(50, 10) * 0.01\n",
        "    B2 = np.random.randn(50, 10) * 0.01\n",
        "    B3 = np.random.randn(50, 10) * 0.01\n",
        "\n",
        "    params = {  \"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2,\n",
        "                \"W3\": W3,\n",
        "                \"b3\": b3,\n",
        "                \"W4\": W4,\n",
        "                \"b4\": b4,\n",
        "                \"B1\": B1,\n",
        "                \"B2\": B2,\n",
        "                \"B3\": B3}\n",
        "\n",
        "    return params\n",
        "\n",
        "def forward_propagation(X, params):\n",
        "    W1 = params[\"W1\"]\n",
        "    b1 = params[\"b1\"]\n",
        "    W2 = params[\"W2\"]\n",
        "    b2 = params[\"b2\"]\n",
        "    W3 = params[\"W3\"]\n",
        "    b3 = params[\"b3\"]\n",
        "    W4 = params[\"W4\"]\n",
        "    b4 = params[\"b4\"]\n",
        "\n",
        "    A1 = np.dot(W1, X) + b1\n",
        "    H1 = np.tanh(A1)\n",
        "    A2 = np.dot(W2, H1) + b2\n",
        "    H2 = np.tanh(A2)\n",
        "    A3 = np.dot(W3, H2) + b3\n",
        "    H3 = np.tanh(A3)\n",
        "    AY = np.dot(W4, H3) + b4\n",
        "    # Y_ = softmax(AY)\n",
        "    Y_ = sigmoid(AY)\n",
        "\n",
        "    cache = {\"A1\": A1,\n",
        "            \"H1\": H1,\n",
        "            \"A2\": A2,\n",
        "            \"H2\": H2,\n",
        "            \"A3\": A3,\n",
        "            \"H3\": H3,\n",
        "            \"AY\": AY,\n",
        "            \"Y_\": Y_}\n",
        "\n",
        "    return Y_, cache\n",
        "\n",
        "def backward_propagation(params, cache, X, Y):\n",
        "    # batch size\n",
        "    N = X.shape[1]\n",
        "\n",
        "    W1 = params[\"W1\"]\n",
        "    W2 = params[\"W2\"]\n",
        "    W3 = params[\"W3\"]\n",
        "    W4 = params[\"W4\"]\n",
        "\n",
        "    H1 = cache[\"H1\"]\n",
        "    H2 = cache[\"H2\"]\n",
        "    H3 = cache[\"H3\"]\n",
        "    Y_ = cache[\"Y_\"]\n",
        "\n",
        "    dY_ = (Y_ - Y)\n",
        "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
        "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
        "\n",
        "    dH3 = np.dot(W4.T, dY_)\n",
        "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
        "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
        "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
        "\n",
        "    dH2 = np.dot(W3.T, dA3)\n",
        "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
        "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
        "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
        "\n",
        "    dH1 = np.dot(W2.T, dA2)\n",
        "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
        "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
        "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dW1\": dW1,\n",
        "                \"db1\": db1,\n",
        "                \"dW2\": dW2,\n",
        "                \"db2\": db2,\n",
        "                \"dW3\": dW3,\n",
        "                \"db3\": db3,\n",
        "                \"dW4\": dW4,\n",
        "                \"db4\": db4}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def direct_feedback_alignment(params, cache, X, Y):\n",
        "    # batch size\n",
        "    N = X.shape[1]\n",
        "\n",
        "    B1 = params[\"B1\"]\n",
        "    B2 = params[\"B2\"]\n",
        "    B3 = params[\"B3\"]\n",
        "\n",
        "    H1 = cache[\"H1\"]\n",
        "    H2 = cache[\"H2\"]\n",
        "    H3 = cache[\"H3\"]\n",
        "    Y_ = cache[\"Y_\"]\n",
        "\n",
        "    dY_ = Y_ - Y\n",
        "    dW4 = (1 / N) * np.dot(dY_, H3.T)\n",
        "    db4 = (1 / N) * np.sum(dY_, axis=1, keepdims=True)\n",
        "\n",
        "    dH3 = np.dot(B3, dY_)\n",
        "    dA3 = dH3 * (1 - np.power(H3, 2))\n",
        "    dW3 = (1 / N) * np.dot(dA3, H2.T)\n",
        "    db3 = (1 / N) * np.sum(dA3, axis=1, keepdims=True)\n",
        "\n",
        "    dH2 = np.dot(B2, dY_)\n",
        "    dA2 = dH2 * (1 - np.power(H2, 2))\n",
        "    dW2 = (1 / N) * np.dot(dA2, H1.T)\n",
        "    db2 = (1 / N) * np.sum(dA2, axis=1, keepdims=True)\n",
        "\n",
        "    dH1 = np.dot(B1, dY_)\n",
        "    dA1 = dH1 * (1 - np.power(H1, 2))\n",
        "    dW1 = (1 / N) * np.dot(dA1, X.T)\n",
        "    db1 = (1 / N) * np.sum(dA1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dW1\": dW1,\n",
        "                \"db1\": db1,\n",
        "                \"dW2\": dW2,\n",
        "                \"db2\": db2,\n",
        "                \"dW3\": dW3,\n",
        "                \"db3\": db3,\n",
        "                \"dW4\": dW4,\n",
        "                \"db4\": db4}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def update_params(params, grads, learning_rate=0.01, is_weights_fixed=False):\n",
        "    if not is_weights_fixed:\n",
        "        params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "        params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "    params[\"W4\"] -= learning_rate * grads[\"dW4\"]\n",
        "    params[\"b4\"] -= learning_rate * grads[\"db4\"]\n",
        "    return params\n",
        "\n",
        "\n",
        "def compute_loss(Y_, Y):\n",
        "\n",
        "    N = Y.shape[1]\n",
        "    # loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)))\n",
        "    loss = (-1 / N) * np.sum(np.multiply(Y, np.log(Y_)) + np.multiply(1 - Y, np.log(1 - Y_)))\n",
        "    loss = np.squeeze(loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(backward_propagation, params, train_loader, test_loader, train_error_list, test_error_list, learning_rate=0.01, start_epoch=0, num_epochs=50, is_weights_fixed=False):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, params)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "            grads = backward_propagation(params, cache, X, Y)\n",
        "            params = update_params(params, grads, learning_rate, is_weights_fixed=is_weights_fixed)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print('Epoch {}, iteration {}, loss : {}, accuracy : {}'.format(start_epoch+epoch+1, i+1, total_loss / 100, total_correct/ total_samples))\n",
        "                total_loss = 0\n",
        "\n",
        "\n",
        "        train_error_rate = (1 - (total_correct / total_samples)) * 100\n",
        "        train_error_list.append(train_error_rate)\n",
        "\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            X = images.view(-1, 28 * 28)\n",
        "            X = np.asarray(X).T\n",
        "            Y = np.eye(10)[labels]\n",
        "            Y = np.asarray(Y).T\n",
        "\n",
        "            labels_gpu = np.asarray(labels)\n",
        "\n",
        "            Y_, cache = forward_propagation(X, params)\n",
        "            loss = compute_loss(Y_, Y)\n",
        "\n",
        "            total_loss += loss\n",
        "            total_correct += np.sum(np.argmax(Y_, axis=0) == labels_gpu)\n",
        "            total_samples += labels_gpu.shape[0]\n",
        "\n",
        "        test_error_rate = (1 - (total_correct / total_samples)) * 100\n",
        "        test_error_list.append(test_error_rate)\n",
        "\n",
        "        print('Test data - After epoch {}, loss : {}, accuracy : {}'.format(start_epoch+epoch+1, total_loss / len(test_loader), total_correct / total_samples))\n",
        "\n",
        "    return params, train_error_list, test_error_list\n",
        "\n",
        "\n",
        "def experiment(params, train_loader, test_loader, learning_rate=0.01):\n",
        "    bp_train_error_list = []\n",
        "    bp_test_error_list = []\n",
        "    dfa_train_error_list = []\n",
        "    dfa_test_error_list = []\n",
        "\n",
        "    params, bp_train_error_list, bp_test_error_list = train(backward_propagation,\n",
        "                                                            params,\n",
        "                                                            train_loader,\n",
        "                                                            test_loader,\n",
        "                                                            bp_train_error_list,\n",
        "                                                            bp_test_error_list,\n",
        "                                                            learning_rate=learning_rate,\n",
        "                                                            num_epochs=50,\n",
        "                                                            is_weights_fixed=True)\n",
        "    bp_params = copy.deepcopy(params)\n",
        "    dfa_params = copy.deepcopy(params)\n",
        "    dfa_train_error_list = copy.deepcopy(bp_train_error_list)\n",
        "    dfa_test_error_list = copy.deepcopy(bp_test_error_list)\n",
        "\n",
        "    bp_params, bp_train_error_list, bp_test_error_list = train(backward_propagation,\n",
        "                                                            bp_params,\n",
        "                                                            train_loader,\n",
        "                                                            test_loader,\n",
        "                                                            bp_train_error_list,\n",
        "                                                            bp_test_error_list,\n",
        "                                                            learning_rate=learning_rate,\n",
        "                                                            start_epoch=50,\n",
        "                                                            num_epochs=50,\n",
        "                                                            is_weights_fixed=False)\n",
        "\n",
        "    dfa_params, dfa_train_error_list, dfa_test_error_list = train(direct_feedback_alignment,\n",
        "                                                            dfa_params,\n",
        "                                                            train_loader,\n",
        "                                                            test_loader,\n",
        "                                                            dfa_train_error_list,\n",
        "                                                            dfa_test_error_list,\n",
        "                                                            learning_rate=learning_rate,\n",
        "                                                            start_epoch=50,\n",
        "                                                            num_epochs=50,\n",
        "                                                            is_weights_fixed=False)\n",
        "\n",
        "\n",
        "\n",
        "    return bp_params, dfa_params, bp_train_error_list, bp_test_error_list, dfa_train_error_list, dfa_test_error_list\n",
        "\n",
        "\n",
        "def predict(X, params):\n",
        "    Y_, cache = forward_propagation(X, params)\n",
        "    predictions = np.argmax(Y_, axis=0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    return np.sum(predictions == labels) / len(labels)"
      ],
      "metadata": {
        "id": "M3HZikVLK-Tw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    batch_size = 64\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "    testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    params = initialize_bp_params()\n",
        ""
      ],
      "metadata": {
        "id": "1lUF7OnuLXIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    bp_params, dfa_params, bp_train_error_list, bp_test_error_list, dfa_train_error_list, dfa_test_error_list = experiment(params, train_loader, test_loader, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "zX2QQcp6UYdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}